{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 25, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,name,category,main_category,currency,deadline,goal,launched,pledged,state,backers,country,usd pledged,usd_pledged_real,usd_goal_real\r",
      "\r\n",
      "1000002330,The Songs of Adelaide & Abullah,Poetry,Publishing,GBP,2015-10-09,1000.00,2015-08-11 12:12:28,0.00,failed,0,GB,0.00,0.00,1533.95\r",
      "\r\n",
      "1000003930,Greeting From Earth: ZGAC Arts Capsule For ET,Narrative Film,Film & Video,USD,2017-11-01,30000.00,2017-09-02 04:43:57,2421.00,failed,15,US,100.00,2421.00,30000.00\r",
      "\r\n",
      "1000004038,Where is Hank?,Narrative Film,Film & Video,USD,2013-02-26,45000.00,2013-01-12 00:20:50,220.00,failed,3,US,220.00,220.00,45000.00\r",
      "\r\n",
      "1000007540,ToshiCapital Rekordz Needs Help to Complete Album,Music,Music,USD,2012-04-16,5000.00,2012-03-17 03:24:11,1.00,failed,1,US,1.00,1.00,5000.00\r",
      "\r\n",
      "1000011046,Community Film Project: The Art of Neighborhood Filmmaking,Film & Video,Film & Video,USD,2015-08-29,19500.00,2015-07-04 08:35:03,1283.00,canceled,14,US,1283.00,1283.00,19500.00\r",
      "\r\n",
      "1000014025,Monarch Espresso Bar,Restaurants,Food,USD,2016-04-01,50000.00,2016-02-26 13:38:27,52375.00,successful,224,US,52375.00,52375.00,50000.00\r",
      "\r\n",
      "1000023410,Support Solar Roasted Coffee & Green Energy!  SolarCoffee.co,Food,Food,USD,2014-12-21,1000.00,2014-12-01 18:30:44,1205.00,successful,16,US,1205.00,1205.00,1000.00\r",
      "\r\n",
      "1000030581,Chaser Strips. Our Strips make Shots their B*tch!,Drinks,Food,USD,2016-03-17,25000.00,2016-02-01 20:05:12,453.00,failed,40,US,453.00,453.00,25000.00\r",
      "\r\n",
      "1000034518,SPIN - Premium Retractable In-Ear Headphones with Mic,Product Design,Design,USD,2014-05-29,125000.00,2014-04-24 18:14:43,8233.00,canceled,58,US,8233.00,8233.00,125000.00\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head ./ks-projects-201801.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378661, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv('./ks-projects-201801.csv')\n",
    "BETTER_COLUMN_NAMES = {\n",
    "    'KickstartedId': 'ID',\n",
    "    'Categoria': 'category',\n",
    "    'CateriaPrincipal': 'main_category',\n",
    "    'FechaLimite': 'deadline',\n",
    "    'Moneda': 'currency',\n",
    "    'Meta': 'goal',\n",
    "    'FechaCreacion': 'launched',\n",
    "    'TotalInvertido': 'pledged',\n",
    "    'Estado': 'state',\n",
    "    'Contribuidores': 'backers',\n",
    "    'Pais': 'country',\n",
    "    'TotalInvertidoEnDolar': 'usd pledged',\n",
    "}\n",
    "all_data.rename(columns=BETTER_COLUMN_NAMES, inplace=True)\n",
    "\n",
    "all_data.set_index('ID', inplace=True)\n",
    "\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['deadline']=pd.to_datetime(all_data['deadline'])\n",
    "all_data['launched']=pd.to_datetime(all_data['launched'])\n",
    "all_data['countday']=all_data['deadline'] - all_data['launched']\n",
    "all_data['countday']= all_data['countday'].astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop(all_data[all_data['state'] == 'live'].index)\n",
    "all_data = all_data.drop(all_data[all_data['state'] == 'suspended'].index)\n",
    "all_data = all_data.drop(all_data[all_data['state'] == 'undefined'].index)\n",
    "all_data = all_data.drop(columns=['name'])\n",
    "all_data = all_data.drop(columns=['category'])\n",
    "all_data = all_data.drop(columns=['usd pledged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['countday'].fillna(all_data.countday.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n",
    "\n",
    "## Salida\n",
    "- state. Valores posibles (suspended,live,undefined,canceled,successfull,state)\n",
    "\n",
    "## Entrada\n",
    "- main_category: categoría. Valores posibles (Music,Publishing,Film & Video,Dance,Journalist,Crafts,Photography,Comics,Theater,Fashion,Food,Art,Desing,Techonogy,Games) string\n",
    "- Countday: Cantidad de dìas entre la diferencia de launched y deadline. float\n",
    "- backers: Patrocinadores\n",
    "- country: País del kickstarter\n",
    "- pledged: invertido.\n",
    "- usd_pledged_real: inversión en dolares\n",
    "- usd_goal_real: Meta convertida en dolar.\n",
    "\n",
    "\n",
    "## No utilizaremos como entradas\n",
    "- ID:  Esto puede generar sobreentrenamiento\n",
    "- name: Esto tiene un gran cambio que puede conducir a sobreentrenamiento.\n",
    "- category: Subcategoría que puede generar duplicidad con el parámetro main_category.\n",
    "- currency: Se duplica de country y por lo tanto puede generar sobreentrenamiento.\n",
    "- usd_pledged: inversión en dolares, contiene valores en null, ademas es igual a usd_pledged_real que no tiene valores en null\n",
    "- deadline y launched: Fecha límite y fecha de lanzamiento del kickstarter se utilizara countday para reflejar la cantidad de dias entre los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% test, 20% validation\n",
    "train, not_train = train_test_split(all_data, test_size=0.4)\n",
    "validation, test = train_test_split(not_train, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, extract_inputs_function, extract_outputs_function, include_validation=False):\n",
    "    sets = [('train', train), ('test', test)]\n",
    "    if include_validation:\n",
    "        sets.append(('validation', validation))\n",
    "        \n",
    "    for set_name, set_data in sets:\n",
    "        inputs = extract_inputs_function(set_data)\n",
    "        outputs = extract_outputs_function(set_data)\n",
    "        predictions = model.predict(inputs)\n",
    "        \n",
    "        print(set_name, '#' * 80)\n",
    "        \n",
    "        # print metrics\n",
    "        \n",
    "        print('Accuracy:', accuracy_score(outputs, predictions))\n",
    "        print('Precision:', precision_score(outputs, predictions))\n",
    "        print('Recall:', recall_score(outputs, predictions))\n",
    "        print()\n",
    "        \n",
    "        # plot confussion matrix\n",
    "        \n",
    "        plt.figure(figsize=(3,4))\n",
    "        \n",
    "        plt.xticks([0, 1, 2], ['successful', 'failed', 'canceled'], rotation=45)\n",
    "        plt.yticks([0, 1, 2], ['successful', 'failed', 'canceled'])\n",
    "        plt.xlabel('Predicted class')\n",
    "        plt.ylabel('True class')\n",
    "\n",
    "        plt.title(set_name)\n",
    "\n",
    "        plt.imshow(\n",
    "            confusion_matrix(outputs, predictions), \n",
    "            cmap=plt.cm.Blues, \n",
    "            interpolation='nearest',\n",
    "        )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370448"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = all_data.copy()\n",
    "ds = ds.drop(ds[ds['main_category'] == None].index)\n",
    "ds['main_category'] = ds.main_category.map({'Film & Video':1,'Music':2,'Publishing':3,'Games':4,'Technology':5,'Design':6,'Art':7,'Food':8,'Fashion':9,'Theater':10,'Comics':11,'Photography':12,'Crafts':13,'Journalism':14,'Dance':15})\n",
    "ds['country'] = ds.country.map({'US':1,'GB':2,'CA':3,'AU':4,'DE':5,'FR':6,'NL':7,'IT':8,'ES':9,'SE':10,'MX':11,'NZ':12,'DK':13,'IE':14,'CH':15,'NO':16,'BE':17,'AT':18,'HK':19,'SG':20,'N,0\"':21,'LU':22,'JP':23})\n",
    "#ds.state.value_counts()  \n",
    "ds = ds.drop(ds[ds['countday'] > 100].index)\n",
    "ds.main_category.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_INPUT_COLUMNS = [\n",
    "    'countday',\n",
    "    'backers',\n",
    "    'country',\n",
    "    'pledged',\n",
    "    'usd_pledged_real',\n",
    "    'usd_goal_real',\n",
    "    'main_category_fv',\n",
    "    'main_category_m',\n",
    "    'main_category_p', \n",
    "    'main_category_g', \n",
    "    'main_category_t', \n",
    "    'main_category_d', \n",
    "    'main_category_a', \n",
    "    'main_category_fd',\n",
    "    'main_category_fh',\n",
    "    'main_category_th',\n",
    "    'main_category_c', \n",
    "    'main_category_ph',\n",
    "    'main_category_cr',\n",
    "    'main_category_j', \n",
    "    'main_category_dc',\n",
    "]\n",
    "\n",
    "\n",
    "def lr_extract_inputs_outputs(dataset):\n",
    "    \"\"\"\n",
    "    Inputs in the format supported by the logistic regressor.\n",
    "    \"\"\"\n",
    "    inputs = dataset.copy()    \n",
    "    \n",
    "    inputs = inputs.drop(inputs[inputs['main_category'] == None].index)\n",
    "    inputs = inputs.drop(inputs[inputs['countday'] > 100].index)\n",
    "    #inputs['main_category'] = inputs.main_category.map({'Film & Video':1,'Music':2,'Publishing':3,'Games':4,'Technology':5,'Design':6,'Art':7,'Food':8,'Fashion':9,'Theater':10,'Comics':11,'Photography':12,'Crafts':13,'Journalism':14,'Dance':15})\n",
    "    inputs['main_category_fv'] = inputs.main_category == 'Film & Video'\n",
    "    inputs['main_category_m'] = inputs.main_category == 'Music'\n",
    "    inputs['main_category_p'] = inputs.main_category == 'Publishing'\n",
    "    inputs['main_category_g'] = inputs.main_category == 'Games'\n",
    "    inputs['main_category_t'] = inputs.main_category == 'Technology'\n",
    "    inputs['main_category_d'] = inputs.main_category == 'Design'\n",
    "    inputs['main_category_a'] = inputs.main_category == 'Art'\n",
    "    inputs['main_category_fd'] = inputs.main_category == 'Food'\n",
    "    inputs['main_category_fh'] = inputs.main_category == 'Fashion'\n",
    "    inputs['main_category_th'] = inputs.main_category == 'Theater'\n",
    "    inputs['main_category_c'] = inputs.main_category == 'Comics'\n",
    "    inputs['main_category_ph'] = inputs.main_category == 'Photography'\n",
    "    inputs['main_category_cr'] = inputs.main_category == 'Crafts'\n",
    "    inputs['main_category_j'] = inputs.main_category == 'Journalism'\n",
    "    inputs['main_category_dc'] = inputs.main_category == 'Dance'\n",
    "\n",
    "    del inputs['main_category']\n",
    "    \n",
    "    inputs['country'] = inputs.country.map({'US':1,'GB':2,'CA':3,'AU':4,'DE':5,'FR':6,'NL':7,'IT':8,'ES':9,'SE':10,'MX':11,'NZ':12,'DK':13,'IE':14,'CH':15,'NO':16,'BE':17,'AT':18,'HK':19,'SG':20,'N,0\"':21,'LU':22,'JP':23})\n",
    "    \n",
    "    #del inputs['country']\n",
    "    \n",
    "    # return the raw numbers\n",
    "    \"\"\"\n",
    "    Outputs in the format supported by the logistic regressor.\n",
    "    \"\"\"\n",
    "    outputs = inputs.copy()    \n",
    " \n",
    "    return [inputs[LR_INPUT_COLUMNS].values.astype(np.float64), outputs.state.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, outputs = lr_extract_inputs_outputs(train)\n",
    "lr_model.fit(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4f8dbe2fb480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_extract_inputs_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2f6d885b4d09>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, extract_inputs_function, extract_outputs_function, include_validation)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_inputs_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_outputs_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "inputs, outputs = lr_extract_inputs_outputs(train)\n",
    "evaluate_model(lr_model, inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "knn_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=K)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.fit(\n",
    "    lr_extract_inputs(train),\n",
    "    lr_extract_outputs(train),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(knn_model, lr_extract_inputs, lr_extract_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
